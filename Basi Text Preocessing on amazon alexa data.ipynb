{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75145323-b4d4-4373-8d6e-b740c13869cf",
   "metadata": {},
   "source": [
    "### Text Preprocessing in NLP\n",
    "Text preprocessing is an essential step in natural language processing (NLP) that involves cleaning and transforming unstructured text data to prepare it for analysis. It includes tokenization, stemming, lemmatization, stop-word removal, and part-of-speech tagging\n",
    "#### Why is Text Preprocessing Important?\n",
    "\r\n",
    "Text preprocessing is crucial in natural language processing (NLP) for several reasonso\n",
    "ns\r\n",
    "Noise Reduct\n",
    "\n",
    "ion\tText data often contains noise such as punctuation, special characters, and irrelevant symbols. Preprocessing helps remove these elements, making the text cleaner and easier to analy\n",
    "ze.\r\n",
    "Normaliza\n",
    "\n",
    "tion\tDifferent forms of words (e.g., “run,” “running,” “ran”) can convey the same meaning but appear in different forms. Preprocessing techniques like stemming and lemmatization help standardize these variati\n",
    "ons.\r\n",
    "Tokeni\n",
    "\n",
    "ation\tText data needs to be broken down into smaller units, such as words or phrases, for analysis. Tokenization divides text into meaningful units, facilitating subsequent processing steps like feature extrac\n",
    "tion.\r\n",
    "Stopword \n",
    "\n",
    "emoval\tStopwords are common words like “the,” “is,” and “and” that often occur frequently but convey little semantic meaning. Removing stopwords can improve the efficiency of text analysis by reducing \n",
    "noise.\r\n",
    "Feature Ex\n",
    "\n",
    "raction\tPreprocessing can involve extracting features from text, such as word frequencies, n-grams, or word embeddings, which are essential for building machine learning#\\ \n",
    "models.\r\n",
    "Dimensionality \n",
    "\n",
    "eduction\tText data often has a high dimensionality due to the presence of a large vocabulary. Preprocessing techniques like term frequency-inverse document frequency (TF-IDF) or dimensionality reduction methods can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e87bd809-0376-4b5f-9b91-26f9a98b9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "import pandas as pd \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc1043f0-ce48-482b-b8ec-3e0dbd8de8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets read the dataset\n",
    "data = pd.read_csv('amazon_alexa.tsv', delimiter = '\\t')\n",
    "# lets check the shape of the dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33d1b48-afdf-456e-bf2f-26db247a1a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Love my Echo!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Loved it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Walnut Finish</td>\n",
       "      <td>Sometimes while playing a game, you can answer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>I have had a lot of fun with this thing. My 4 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Music</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating       date         variation  \\\n",
       "0       5  31-Jul-18  Charcoal Fabric    \n",
       "1       5  31-Jul-18  Charcoal Fabric    \n",
       "2       4  31-Jul-18    Walnut Finish    \n",
       "3       5  31-Jul-18  Charcoal Fabric    \n",
       "4       5  31-Jul-18  Charcoal Fabric    \n",
       "\n",
       "                                    verified_reviews  feedback  \n",
       "0                                      Love my Echo!         1  \n",
       "1                                          Loved it!         1  \n",
       "2  Sometimes while playing a game, you can answer...         1  \n",
       "3  I have had a lot of fun with this thing. My 4 ...         1  \n",
       "4                                              Music         1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7abb5c87-8d7f-4708-bdd3-22cb38081feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variation\n",
       "Black  Dot                      516\n",
       "Charcoal Fabric                 430\n",
       "Configuration: Fire TV Stick    350\n",
       "Black  Plus                     270\n",
       "Black  Show                     265\n",
       "Black                           261\n",
       "Black  Spot                     241\n",
       "White  Dot                      184\n",
       "Heather Gray Fabric             157\n",
       "White  Spot                     109\n",
       "White                            91\n",
       "Sandstone Fabric                 90\n",
       "White  Show                      85\n",
       "White  Plus                      78\n",
       "Oak Finish                       14\n",
       "Walnut Finish                     9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check the Value Counts for Variation \n",
    "data['variation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e63ffa2f-20a4-48ce-99ae-fb30ad11e97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        12\n",
       "1         8\n",
       "2       192\n",
       "3       167\n",
       "4         5\n",
       "       ... \n",
       "3145     47\n",
       "3146    129\n",
       "3147    423\n",
       "3148    371\n",
       "3149      4\n",
       "Name: length, Length: 3150, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets calculate the length of the Reviews\n",
    "data['length'] = data['verified_reviews'].apply(len)\n",
    "data['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8754ed9-3be7-4cf2-9fcb-36a7ac9761c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        12\n",
       "1         8\n",
       "2       192\n",
       "3       167\n",
       "4         5\n",
       "       ... \n",
       "3145     47\n",
       "3146    129\n",
       "3147    423\n",
       "3148    371\n",
       "3149      4\n",
       "Name: char_count, Length: 3150, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating the Character Count in the Reviews\n",
    "data['char_count'] = data['verified_reviews'].apply(len)\n",
    "data['char_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ea1920e-dcac-40a0-9252-2d4d1316c5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        3\n",
       "1        2\n",
       "2       38\n",
       "3       33\n",
       "4        1\n",
       "        ..\n",
       "3145     8\n",
       "3146    23\n",
       "3147    83\n",
       "3148    76\n",
       "3149     1\n",
       "Name: word_count, Length: 3150, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating the Word Count\n",
    "data['word_count'] = data['verified_reviews'].apply(lambda x: len(x.split()))\n",
    "data['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c258b48d-0149-459c-b01b-6c7e5c00897a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "3145    1\n",
       "3146    1\n",
       "3147    1\n",
       "3148    1\n",
       "3149    1\n",
       "Name: sentence_count, Length: 3150, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# Function to count words in a text\n",
    "def count_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)  # Matches word boundaries\n",
    "    return len(words)\n",
    "# Function to count sentences in a text\n",
    "def count_sentences(text):\n",
    "    sentences = re.split(r'[.!?]+', text)  # Split on '.', '!', or '?'\n",
    "    sentences = [s for s in sentences if s.strip()]  # Remove empty strings\n",
    "    return len(sentences)\n",
    "# Apply functions to the DataFrame\n",
    "data['word_count'] = data['verified_reviews'].apply(lambda x: count_words(x))\n",
    "data['sentence_count'] = data['verified_reviews'].apply(lambda x: count_sentences(x))\n",
    "data['word_count']\n",
    "data['sentence_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1363952-f2af-4476-a056-28bb4fb94470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      rating       date         variation  \\\n",
      "0          5  31-Jul-18  Charcoal Fabric    \n",
      "1          5  31-Jul-18  Charcoal Fabric    \n",
      "2          4  31-Jul-18    Walnut Finish    \n",
      "3          5  31-Jul-18  Charcoal Fabric    \n",
      "4          5  31-Jul-18  Charcoal Fabric    \n",
      "...      ...        ...               ...   \n",
      "3145       5  30-Jul-18        Black  Dot   \n",
      "3146       5  30-Jul-18        Black  Dot   \n",
      "3147       5  30-Jul-18        Black  Dot   \n",
      "3148       5  30-Jul-18        White  Dot   \n",
      "3149       4  29-Jul-18        Black  Dot   \n",
      "\n",
      "                                       verified_reviews  feedback  length  \\\n",
      "0                                          Love my Echo         1      12   \n",
      "1                                              Loved it         1       8   \n",
      "2     Sometimes while playing a game you can answer ...         1     192   \n",
      "3     I have had a lot of fun with this thing My  yr...         1     167   \n",
      "4                                                 Music         1       5   \n",
      "...                                                 ...       ...     ...   \n",
      "3145    Perfect for kids adults and everyone in between         1      47   \n",
      "3146  Listening to music searching locations checkin...         1     129   \n",
      "3147  I do love these things i have them running my ...         1     423   \n",
      "3148  Only complaint I have is that the sound qualit...         1     371   \n",
      "3149                                               Good         1       4   \n",
      "\n",
      "      char_count  word_count  sentence_count  \n",
      "0             12           3               1  \n",
      "1              8           2               1  \n",
      "2            192          38               1  \n",
      "3            167          33               1  \n",
      "4              5           1               1  \n",
      "...          ...         ...             ...  \n",
      "3145          47           8               1  \n",
      "3146         129          23               1  \n",
      "3147         423          83               1  \n",
      "3148         371          76               1  \n",
      "3149           4           1               1  \n",
      "\n",
      "[3150 rows x 9 columns]\n",
      "\n",
      "Total Words: 79531\n",
      "Total Sentences: 3065\n"
     ]
    }
   ],
   "source": [
    "# Calculate totals\n",
    "total_words = data['word_count'].sum()\n",
    "total_sentences = data['sentence_count'].sum()\n",
    "# Display results\n",
    "print(data)\n",
    "print(f\"\\nTotal Words: {total_words}\")\n",
    "print(f\"Total Sentences: {total_sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b17eb-328b-458a-add6-b395a18b84b6",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed575a74-d13e-4472-beb2-cdb7966f0a6a",
   "metadata": {},
   "source": [
    "#### 1.Punctuations remove\n",
    "First lets remove Punctuations from the Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45b2885d-7cd1-4389-983a-3734e091d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets remove Punctuations from the Reviews\n",
    "def punctuation_removal(messy_str):\n",
    "    clean_list = [char for char in messy_str if char not in string.punctuation]\n",
    "    clean_str = ''.join(clean_list)\n",
    "    return clean_str\n",
    "\n",
    "data['verified_reviews'] = data['verified_reviews'].apply(punctuation_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38b1433a-7c70-483e-8f84-5aac4d37f987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating              0\n",
       "date                0\n",
       "variation           0\n",
       "verified_reviews    1\n",
       "feedback            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76608370-6d3a-44d6-9f71-c10a62f74ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      rating       date         variation  \\\n",
      "0          5  31-Jul-18  Charcoal Fabric    \n",
      "1          5  31-Jul-18  Charcoal Fabric    \n",
      "2          4  31-Jul-18    Walnut Finish    \n",
      "3          5  31-Jul-18  Charcoal Fabric    \n",
      "4          5  31-Jul-18  Charcoal Fabric    \n",
      "...      ...        ...               ...   \n",
      "3145       5  30-Jul-18        Black  Dot   \n",
      "3146       5  30-Jul-18        Black  Dot   \n",
      "3147       5  30-Jul-18        Black  Dot   \n",
      "3148       5  30-Jul-18        White  Dot   \n",
      "3149       4  29-Jul-18        Black  Dot   \n",
      "\n",
      "                                       verified_reviews  feedback  \n",
      "0                                          Love my Echo         1  \n",
      "1                                              Loved it         1  \n",
      "2     Sometimes while playing a game you can answer ...         1  \n",
      "3     I have had a lot of fun with this thing My 4 y...         1  \n",
      "4                                                 Music         1  \n",
      "...                                                 ...       ...  \n",
      "3145    Perfect for kids adults and everyone in between         1  \n",
      "3146  Listening to music searching locations checkin...         1  \n",
      "3147  I do love these things i have them running my ...         1  \n",
      "3148  Only complaint I have is that the sound qualit...         1  \n",
      "3149                                               Good         1  \n",
      "\n",
      "[3150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "def punctuation_removal(messy_str):\n",
    "    if isinstance(messy_str, str):  # Check if input is a string\n",
    "        clean_list = [char for char in messy_str if char not in string.punctuation]\n",
    "        clean_str = ''.join(clean_list)\n",
    "        return clean_str\n",
    "    return messy_str  # Return non-string values as is\n",
    "# Apply the function to the DataFrame\n",
    "data['verified_reviews'] = data['verified_reviews'].apply(punctuation_removal)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d87bf-7747-4e6c-b661-048ceffcb110",
   "metadata": {},
   "source": [
    "#### 2.Number Remove\n",
    "lets make a function to remove Numbers from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6193bab-ad0c-4377-8114-4f4e2b76cf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                            Love my Echo\n",
       "1                                                Loved it\n",
       "2       Sometimes while playing a game you can answer ...\n",
       "3       I have had a lot of fun with this thing My  yr...\n",
       "4                                                   Music\n",
       "                              ...                        \n",
       "3145      Perfect for kids adults and everyone in between\n",
       "3146    Listening to music searching locations checkin...\n",
       "3147    I do love these things i have them running my ...\n",
       "3148    Only complaint I have is that the sound qualit...\n",
       "3149                                                 Good\n",
       "Name: verified_reviews, Length: 3150, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.lets make a function to remove Numbers from the reviews\n",
    "import re\n",
    "def drop_numbers(list_text):\n",
    "    list_text_new = []\n",
    "    for i in list_text:\n",
    "        if not re.search('\\d', i):\n",
    "            list_text_new.append(i)\n",
    "    return ''.join(list_text_new)\n",
    "\n",
    "data['verified_reviews'] = data['verified_reviews'].apply(drop_numbers)\n",
    "data['verified_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c43ecddd-48c2-484a-a8c4-1550a9a220cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating              0\n",
       "date                0\n",
       "variation           0\n",
       "verified_reviews    1\n",
       "feedback            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18ad15b6-724e-4b24-b734-9c6e56c94a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['verified_reviews'].fillna('', inplace=True)  # Replace NaN with an empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f219fb09-6d1a-428f-bf89-fa8540cf3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.lets make a function to remove Numbers from the reviews\n",
    "import re\n",
    "def drop_numbers(list_text):\n",
    "    list_text_new = []\n",
    "    for i in list_text:\n",
    "        if not re.search('\\d', i):\n",
    "            list_text_new.append(i)\n",
    "    return ''.join(list_text_new)\n",
    "data['verified_reviews'] = data['verified_reviews'].apply(drop_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "424ea6fe-0acf-4d63-8b4d-6edf398c6539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                         Love my Echo\n",
       "1                                             Loved it\n",
       "2    Sometimes while playing a game you can answer ...\n",
       "3    I have had a lot of fun with this thing My  yr...\n",
       "4                                                Music\n",
       "5    I received the echo as a gift I needed another...\n",
       "6    Without having a cellphone I cannot use many o...\n",
       "7    I think this is the th one Ive purchased Im wo...\n",
       "8                                          looks great\n",
       "9    Love it I’ve listened to songs I haven’t heard...\n",
       "Name: verified_reviews, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['verified_reviews'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2e41d-1204-41ae-8658-d3d6e0192900",
   "metadata": {},
   "source": [
    "#### 4.Removing Special Characters\n",
    "\n",
    "Special characters, as you know, are non-alphanumeric characters. These characters are most often found in comments, references, currency numbers etc. These characters add no value to text-understanding and induce noise into algorithms. Thankfully, regular-expressions (regex) can be used to get rid of these characters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7a2cd42-b80e-4330-b66b-5d600186564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    pat = r'[^a-zA-z0-9]' \n",
    "    return re.sub(pat, ' ', text)\n",
    " \n",
    "# lets apply this function\n",
    "data['verified_reviews'] = data.apply(lambda x: remove_special_characters(x['verified_reviews']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56274152-50bf-4a5a-9074-cc6eee48e155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                         Love my Echo\n",
       "1                                             Loved it\n",
       "2    Sometimes while playing a game you can answer ...\n",
       "3    I have had a lot of fun with this thing My  yr...\n",
       "4                                                Music\n",
       "Name: verified_reviews, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lets check the Head of Verified Reviews After Cleaning\n",
    "data['verified_reviews'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eda7b0-065d-4643-b7ca-6f561f43eafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7cd9e08-692a-490e-8bf3-6da30916239a",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "In Python tokenization basically refers to splitting up a larger body of text into smaller lines, words or even creating words for a non-English language. The various tokenization functions in-built into the nltk module.\n",
    "These tokens can be as small as characters or as long as words. The primary reason this process matters is that it helps machines understand human language by breaking it down into bite-sized pieces, which are easier to analyze.\n",
    "Imagine you're trying to teach a child to read. Instead of diving straight into complex paragraphs, you'd start by introducing them to individual letters, then syllables, and finally, whole words. In a similar vein, tokenization breaks down vast stretches of text into more digestible and understandable units for machines.\r\n",
    "\r\n",
    "The primary goal of tokenization is to represent text in a manner that's meaningful for machines without losing its context. By converting text into tokens, algorithms can more easily identify patterns. This pattern recognition is crucial because it makes it possible for machines to understand and respond to human input. For instance, when a machine encounters the word \"running\", it doesn't see it as a singular entity but rather as a combination of tokens that it can analyze and derive meaning from.\r\n",
    "\r\n",
    "To delve deeper into the mechanics, consider the sentence, \"Chatbots are helpful.\" When we tokenize this sentence by words, it transforms into an array of individual words:\r\n",
    "\r\n",
    "[\"Chatbots\", \"are\", \"helpful\"].\r\n",
    "\r\n",
    "This is a straightforward approach where spaces typically dictate the boundaries of tokens. However, if we were to tokenize by characters, the sentence would fragment into:\r\n",
    "\r\n",
    "[\"C\", \"h\", \"a\", \"t\", \"b\", \"o\", \"t\", \"s\", \" \", \"a\", \"r\", \"e\", \" \", \"h\", \"e\", \"l\", \"p\", \"f\", \"u\", \"l\"].\r\n",
    "\r\n",
    "This character-level breakdown is more granular and can be especially useful for certain languages or specific NLP tasks.\r\n",
    "\r\n",
    "In essence, tokenization is akin to dissecting a sentence to understand its anatomy. Just as doctors study individual cells to understand an organ, NLP practitioners use tokenization to dissect and understand the structure and me\n",
    "\n",
    "Tokenization methods vary based on the granularity of the text breakdown and the specific requirements of the task at hand. These methods can range from dissecting text into individual words to breaking them down into characters or even smaller units. Here's a closer look at the different types:\r\n",
    "\r\n",
    "Word tokenization. This method breaks text down into individual words. It's the most common approach and is particularly effective for languages with clear word boundaries like English.\r\n",
    "Character tokenization. Here, the text is segmented into individual characters. This method is beneficial for languages that lack clear word boundaries or for tasks that require a granular analysis, such as spelling correction.\r\n",
    "Subword tokenization. Striking a balance between word and character tokenization, this method breaks text into units that might be larger than a single character but smaller than a full word. For instance, \"Chatbots\" could be tokenized into \"Chat\" and \"bots\". This approach is especially useful for languages that form meaning by combining smaller units or when dealing with out-of-vocabulary words in NLP tasks.\r\n",
    "Here's a table explaining the differences: \r\n",
    "\r\n",
    "\r\n",
    "Type\tDescription\tUse Cases\r\n",
    "Word Tokenization\tBreaks text into individual words.\tEffective for languages with clear word boundaries like English.\r\n",
    "Character Tokenization\tSegments text into individual characters.\tUseful for languages without clear word boundaries or tasks requiring granular analysis.\r\n",
    "Subword Tokenization\tBreaks text into units larger than characters but smaller than words.\tBeneficial for languages with complex morphology or handling out-of-vocabu\n",
    "\n",
    "lary words.aning of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0651377-f028-4a28-9ff6-2ad9768f9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13eb4637-9fb1-424d-9d9b-8e1ba7d80292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The First sentence is about Python.', 'The Second: about Django.', 'You can learn Python, Django and Data Ananlysis here.']\n"
     ]
    }
   ],
   "source": [
    "# sentence Tokenizations\n",
    "sentence_data = \"The First sentence is about Python. The Second: about Django. You can learn Python, \\\n",
    "Django and Data Ananlysis here. \"\n",
    "\n",
    "nltk_tokens = nltk.sent_tokenize(sentence_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d81357fc-ce1c-41a0-9bc6-f84117ca3b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wie geht es Ihnen?', 'Gut, danke.']\n"
     ]
    }
   ],
   "source": [
    "# Non English Tokenization\n",
    "german_tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "\n",
    "german_tokens=german_tokenizer.tokenize('Wie geht es Ihnen?  Gut, danke.')\n",
    "print(german_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56d632d1-794b-408c-a037-106393d7aa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'originated', 'from', 'the', 'idea', 'that', 'there', 'are', 'readers', 'who', 'prefer', 'learning', 'new', 'skills', 'from', 'the', 'comforts', 'of', 'their', 'drawing', 'rooms']\n"
     ]
    }
   ],
   "source": [
    "# Words Tokenization\n",
    "word_data = \"It originated from the idea that there are readers who prefer learning new \\\n",
    "skills from the comforts of their drawing rooms\"\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8daa255-4150-41ca-8840-fc7acf4a36f7",
   "metadata": {},
   "source": [
    "### 3 Stopwords\n",
    "\n",
    "Stop words are a set of commonly used words in any language. For example, in English, “the”, “is” and “and”, would easily qualify as stop words. In NLP and text mining applications, stop words are used to eliminate unimportant words, allowing applications to focus on the important words instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60dbe3e3-2112-40e6-99e5-16aed0741a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for stopwords Removal\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8deac4c-c7c0-42fb-aca9-9f5f786fdc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# lets print the Stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4595cbca-f54a-46c8-8404-8064fa89f490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا', 'أبٌ', 'أخٌ', 'حمٌ', 'فو', 'أنتِ', 'يناير', 'فبراير', 'مارس', 'أبريل', 'مايو', 'يونيو', 'يوليو', 'أغسطس', 'سبتمبر', 'أكتوبر', 'نوفمبر', 'ديسمبر', 'جانفي', 'فيفري', 'مارس', 'أفريل', 'ماي', 'جوان', 'جويلية', 'أوت', 'كانون', 'شباط', 'آذار', 'نيسان', 'أيار', 'حزيران', 'تموز', 'آب', 'أيلول', 'تشرين', 'دولار', 'دينار', 'ريال', 'درهم', 'ليرة', 'جنيه', 'قرش', 'مليم', 'فلس', 'هللة', 'سنتيم', 'يورو', 'ين', 'يوان', 'شيكل', 'واحد', 'اثنان', 'ثلاثة', 'أربعة', 'خمسة', 'ستة', 'سبعة', 'ثمانية', 'تسعة', 'عشرة', 'أحد', 'اثنا', 'اثني', 'إحدى', 'ثلاث', 'أربع', 'خمس', 'ست', 'سبع', 'ثماني', 'تسع', 'عشر', 'ثمان', 'سبت', 'أحد', 'اثنين', 'ثلاثاء', 'أربعاء', 'خميس', 'جمعة', 'أول', 'ثان', 'ثاني', 'ثالث', 'رابع', 'خامس', 'سادس', 'سابع', 'ثامن', 'تاسع', 'عاشر', 'حادي', 'أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'ى', 'آ', 'ؤ', 'ئ', 'أ', 'ة', 'ألف', 'باء', 'تاء', 'ثاء', 'جيم', 'حاء', 'خاء', 'دال', 'ذال', 'راء', 'زاي', 'سين', 'شين', 'صاد', 'ضاد', 'طاء', 'ظاء', 'عين', 'غين', 'فاء', 'قاف', 'كاف', 'لام', 'ميم', 'نون', 'هاء', 'واو', 'ياء', 'همزة', 'ي', 'نا', 'ك', 'كن', 'ه', 'إياه', 'إياها', 'إياهما', 'إياهم', 'إياهن', 'إياك', 'إياكما', 'إياكم', 'إياك', 'إياكن', 'إياي', 'إيانا', 'أولالك', 'تانِ', 'تانِك', 'تِه', 'تِي', 'تَيْنِ', 'ثمّ', 'ثمّة', 'ذانِ', 'ذِه', 'ذِي', 'ذَيْنِ', 'هَؤلاء', 'هَاتانِ', 'هَاتِه', 'هَاتِي', 'هَاتَيْنِ', 'هَذا', 'هَذانِ', 'هَذِه', 'هَذِي', 'هَذَيْنِ', 'الألى', 'الألاء', 'أل', 'أنّى', 'أيّ', 'ّأيّان', 'أنّى', 'أيّ', 'ّأيّان', 'ذيت', 'كأيّ', 'كأيّن', 'بضع', 'فلان', 'وا', 'آمينَ', 'آهِ', 'آهٍ', 'آهاً', 'أُفٍّ', 'أُفٍّ', 'أفٍّ', 'أمامك', 'أمامكَ', 'أوّهْ', 'إلَيْكَ', 'إلَيْكَ', 'إليكَ', 'إليكنّ', 'إيهٍ', 'بخٍ', 'بسّ', 'بَسْ', 'بطآن', 'بَلْهَ', 'حاي', 'حَذارِ', 'حيَّ', 'حيَّ', 'دونك', 'رويدك', 'سرعان', 'شتانَ', 'شَتَّانَ', 'صهْ', 'صهٍ', 'طاق', 'طَق', 'عَدَسْ', 'كِخ', 'مكانَك', 'مكانَك', 'مكانَك', 'مكانكم', 'مكانكما', 'مكانكنّ', 'نَخْ', 'هاكَ', 'هَجْ', 'هلم', 'هيّا', 'هَيْهات', 'وا', 'واهاً', 'وراءَك', 'وُشْكَانَ', 'وَيْ', 'يفعلان', 'تفعلان', 'يفعلون', 'تفعلون', 'تفعلين', 'اتخذ', 'ألفى', 'تخذ', 'ترك', 'تعلَّم', 'جعل', 'حجا', 'حبيب', 'خال', 'حسب', 'خال', 'درى', 'رأى', 'زعم', 'صبر', 'ظنَّ', 'عدَّ', 'علم', 'غادر', 'ذهب', 'وجد', 'ورد', 'وهب', 'أسكن', 'أطعم', 'أعطى', 'رزق', 'زود', 'سقى', 'كسا', 'أخبر', 'أرى', 'أعلم', 'أنبأ', 'حدَث', 'خبَّر', 'نبَّا', 'أفعل به', 'ما أفعله', 'بئس', 'ساء', 'طالما', 'قلما', 'لات', 'لكنَّ', 'ءَ', 'أجل', 'إذاً', 'أمّا', 'إمّا', 'إنَّ', 'أنًّ', 'أى', 'إى', 'أيا', 'ب', 'ثمَّ', 'جلل', 'جير', 'رُبَّ', 'س', 'علًّ', 'ف', 'كأنّ', 'كلَّا', 'كى', 'ل', 'لات', 'لعلَّ', 'لكنَّ', 'لكنَّ', 'م', 'نَّ', 'هلّا', 'وا', 'أل', 'إلّا', 'ت', 'ك', 'لمّا', 'ن', 'ه', 'و', 'ا', 'ي', 'تجاه', 'تلقاء', 'جميع', 'حسب', 'سبحان', 'شبه', 'لعمر', 'مثل', 'معاذ', 'أبو', 'أخو', 'حمو', 'فو', 'مئة', 'مئتان', 'ثلاثمئة', 'أربعمئة', 'خمسمئة', 'ستمئة', 'سبعمئة', 'ثمنمئة', 'تسعمئة', 'مائة', 'ثلاثمائة', 'أربعمائة', 'خمسمائة', 'ستمائة', 'سبعمائة', 'ثمانمئة', 'تسعمائة', 'عشرون', 'ثلاثون', 'اربعون', 'خمسون', 'ستون', 'سبعون', 'ثمانون', 'تسعون', 'عشرين', 'ثلاثين', 'اربعين', 'خمسين', 'ستين', 'سبعين', 'ثمانين', 'تسعين', 'بضع', 'نيف', 'أجمع', 'جميع', 'عامة', 'عين', 'نفس', 'لا سيما', 'أصلا', 'أهلا', 'أيضا', 'بؤسا', 'بعدا', 'بغتة', 'تعسا', 'حقا', 'حمدا', 'خلافا', 'خاصة', 'دواليك', 'سحقا', 'سرا', 'سمعا', 'صبرا', 'صدقا', 'صراحة', 'طرا', 'عجبا', 'عيانا', 'غالبا', 'فرادى', 'فضلا', 'قاطبة', 'كثيرا', 'لبيك', 'معاذ', 'أبدا', 'إزاء', 'أصلا', 'الآن', 'أمد', 'أمس', 'آنفا', 'آناء', 'أنّى', 'أول', 'أيّان', 'تارة', 'ثمّ', 'ثمّة', 'حقا', 'صباح', 'مساء', 'ضحوة', 'عوض', 'غدا', 'غداة', 'قطّ', 'كلّما', 'لدن', 'لمّا', 'مرّة', 'قبل', 'خلف', 'أمام', 'فوق', 'تحت', 'يمين', 'شمال', 'ارتدّ', 'استحال', 'أصبح', 'أضحى', 'آض', 'أمسى', 'انقلب', 'بات', 'تبدّل', 'تحوّل', 'حار', 'رجع', 'راح', 'صار', 'ظلّ', 'عاد', 'غدا', 'كان', 'ما انفك', 'ما برح', 'مادام', 'مازال', 'مافتئ', 'ابتدأ', 'أخذ', 'اخلولق', 'أقبل', 'انبرى', 'أنشأ', 'أوشك', 'جعل', 'حرى', 'شرع', 'طفق', 'علق', 'قام', 'كرب', 'كاد', 'هبّ']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('arabic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56c8aada-4fb8-4a16-9c2a-0773e9d0e2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nick', 'likes', 'play', 'football', ',', 'fond', 'tennis', '.']\n"
     ]
    }
   ],
   "source": [
    "#Now lets Remove the Stopwords\n",
    "# targeting only English Stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop_words = []\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ad62cc1-fb8b-480d-ad1e-e5fae76e244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nick likes play football, fond tennis.\n"
     ]
    }
   ],
   "source": [
    "# using gensim to remove stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "filtered_sentence = remove_stopwords(text)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a65a91-e9f9-4739-838e-641159ec1247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96eb554d-2e0e-476d-baa6-8055e8b9cb20",
   "metadata": {},
   "source": [
    "### 4.Stemming\n",
    "\n",
    "Stemming is the process of reducing inflected/derived words to their word stem, base or root form. The stem need not be identical to original word. There are many ways to perform stemming such as lookup table, suffix-stripping algorithms etc. These mainly rely on chopping-off ‘s’, ‘es’, ‘ed’, ‘ing’, ‘ly’ etc from the end of the words and sometimes the conversion is not desirable. But nonetheless, stemming helps us in standardizing tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c708c26a-8ff4-4a5c-affd-5001e7634682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we are eat and swim ; we have been eat and swim ; he eat and swim ; he ate and swam'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for stemming\n",
    "def get_stem(text):\n",
    "    stemmer = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "# call function\n",
    "get_stem(\"we are eating and swimming ; we have been eating and swimming ; he eats and swims ; he ate and swam \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a026e79-e563-4b88-87cf-dc4c3870d080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter | Lancaster\n",
      "happi | happy\n",
      "happiest | happiest\n",
      "happier | happy\n",
      "cactu | cact\n",
      "cactii | cacti\n",
      "eleph | eleph\n",
      "eleph | eleph\n",
      "amaz | amaz\n",
      "amaz | amaz\n",
      "amazingli | amaz\n",
      "cement | cem\n",
      "owe | ow\n",
      "maximum | maxim\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = ['happy', 'happiest', 'happier', 'cactus', 'cactii', 'elephant', 'elephants', 'amazed', 'amazing', 'amazingly', 'cement', 'owed', 'maximum']\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "stemmed = [(porter.stem(word), lancaster.stem(word)) for word in words_to_stem]\n",
    "\n",
    "print(\"Porter | Lancaster\")\n",
    "for stem in stemmed:\n",
    "    print(f\"{stem[0]} | {stem[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322821dd-bd44-4e73-a1d9-2fd06105e676",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Though stemming and lemmatization both generate the root form of inflected/desired words, but lemmatization is an advanced form of stemming. Stemming might not result in actual word, whereas lemmatization does conversion properly with the use of vocabulary, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "Before using lemmatization, we should be aware that it is considerably slower than stemming, so performance should be kept in mind before choosing stemming or lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfd1d90a-a14f-45bf-80e3-317094b163fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5296285c-eaef-4b29-986e-0459acfdaec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'e',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 's',\n",
       " 'w',\n",
       " 'i',\n",
       " 'm',\n",
       " 'm',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " ';',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 'e',\n",
       " 'n',\n",
       " ' ',\n",
       " 'e',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 's',\n",
       " 'w',\n",
       " 'i',\n",
       " 'm',\n",
       " 'm',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " ';',\n",
       " ' ',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'e',\n",
       " 'a',\n",
       " 't',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 's',\n",
       " 'w',\n",
       " 'i',\n",
       " 'm',\n",
       " 's',\n",
       " ' ',\n",
       " ';',\n",
       " ' ',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 's',\n",
       " 'w',\n",
       " 'a',\n",
       " 'm',\n",
       " ' ']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def lemmatizer(text1):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text1]\n",
    "    return lemm_text\n",
    "\n",
    "# call function\n",
    "lemmatizer(\"we are eating and swimming ; we have been eating and swimming ; he eats and swims ; he ate and swam \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95ed836-00be-4f4b-8ed6-f84db3fd9377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASIM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['amaze', 'amazed', 'amazing']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['amaze', 'amazed', 'amazing']\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "[lemmatizer.lemmatize(word) for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a494cdd-b3f5-49cf-8435-f42cfcdbe47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amaze', 'amaze', 'amaze']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "[lemmatizer.lemmatize(word, wordnet.VERB) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f3d03-c3f1-490f-91d4-01d3c8665b25",
   "metadata": {},
   "source": [
    "### 5.POS (Part-of-Speech) Tagging\n",
    "Definition: POS tagging is the process of assigning a part of speech (such as noun, verb, adjective, etc.) to each word in a given text based on its definition and context.\n",
    "\n",
    "We can implement POS tagging in Python using the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b8d6621-4c86-4bad-94fa-e09aa2b14f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagged Tokens:\n",
      "We -> PRP\n",
      "are -> VBP\n",
      "eating -> VBG\n",
      "and -> CC\n",
      "swimming -> VBG\n",
      "; -> :\n",
      "we -> PRP\n",
      "have -> VBP\n",
      "been -> VBN\n",
      "eating -> VBG\n",
      "and -> CC\n",
      "swimming -> NN\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample text\n",
    "text = \"We are eating and swimming; we have been eating and swimming.\"\n",
    "\n",
    "# Step 1: Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Step 2: Perform POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Display the results\n",
    "print(\"POS Tagged Tokens:\")\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"{word} -> {tag}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
